\documentclass{article}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

\author{}
\date{}

\begin{document}
\maketitle {}

\section{Entropy-Based Random Forest Compression}

\subsection{Introduction}

The rapid growth of data in various fields has led to challenges in storing, processing, and analyzing large datasets. Machine learning models trained on such datasets often face computational bottlenecks and increased training times, which can hinder their deployment in real-time or resource-constrained environments. Dataset compression techniques aim to reduce the size of the dataset while preserving essential information, thereby improving computational efficiency without significantly compromising model performance.

In this section, we introduce an entropy-based Random Forest compression method that leverages predictive uncertainties to select the most informative samples from the original dataset. By focusing on samples with high and low predictive entropy, we aim to create a compressed dataset that maintains the integrity of the original data distribution and supports high model accuracy.

\subsection{Background and Theoretical Foundations}

\subsubsection{Random Forest Overview}

Random Forests, introduced by Breiman \cite{breiman2001random}, are ensemble learning methods that construct multiple decision trees during training and output the mode of the classes (classification) or mean prediction (regression) of the individual trees. Each tree is built from a bootstrap sample of the data, and at each split, a random subset of features is considered, which introduces diversity among the trees and reduces overfitting.

Random Forests are known for their ability to handle high-dimensional data, robustness to noise, and effective estimation of feature importance. They have been successfully applied in various domains, including bioinformatics \cite{diaz2006gene}, text classification \cite{liu2011effective}, and imbalanced datasets \cite{homjandee2021random}.

\subsubsection{Entropy in Machine Learning}

Entropy, a concept from information theory introduced by Shannon \cite{shannon1948mathematical}, measures the uncertainty or unpredictability of a random variable. In machine learning, entropy is used in decision trees to quantify the impurity of a node. The entropy $H$ of a discrete random variable $Y$ with possible values $\{y_1, y_2, ..., y_n\}$ and probability mass function $P(Y)$ is defined as:

\begin{equation}
H(Y) = -\sum_{i=1}^{n} P(y_i) \log_2 P(y_i)
\end{equation}

A higher entropy value indicates greater uncertainty, while lower entropy signifies more certainty in the outcomes.

\subsubsection{Predictive Entropy}

Predictive entropy quantifies the uncertainty in a model's predictions. For a classification model that outputs class probabilities, the predictive entropy for a sample $x_i$ is calculated using the predicted probabilities $P(y_k|x_i)$ for each class $k$:

\begin{equation}
H_i = -\sum_{k=1}^{K} P(y_k|x_i) \log_2 P(y_k|x_i)
\end{equation}

where $K$ is the total number of classes. High predictive entropy indicates that the model is uncertain about its prediction for that sample, while low entropy suggests confidence.

Predictive entropy is widely used in active learning to select informative samples for labeling \cite{settles2009active}. By focusing on samples with high uncertainty, models can learn more efficiently from fewer data points.

\subsection{Methodology}

\subsubsection{Overview of the Compression Technique}

The entropy-based Random Forest compression technique aims to reduce the dataset size by selecting a subset of samples that are most informative for model training. The key idea is to retain samples that the model predicts with either high certainty (low entropy) or high uncertainty (high entropy). These samples help the model reinforce its confident predictions and improve its understanding of uncertain areas in the feature space.

The compression process involves the following steps:

\begin{enumerate}
    \item Data preparation and normalization.
    \item Training an initial Random Forest model on the full dataset.
    \item Calculating predictive entropy for each training sample.
    \item Selecting samples based on high and low entropy thresholds.
    \item Ensuring class balance in the selected core dataset.
    \item Further compressing the core dataset using stratified sampling.
\end{enumerate}

\subsubsection{Detailed Steps}

\paragraph{Data Preparation}

The dataset consists of sensor readings from multiple nodes and corresponding application labels. Data preparation involves:

\begin{itemize}
    \item Loading and merging sensor data with response labels.
    \item Mapping application labels to numerical values for classification.
    \item Pivoting the data to create a wide format where each row represents a timestamp and columns represent sensor features from different nodes.
    \item Normalizing features using standard scaling to ensure that all features contribute equally to the model.
    \item Applying variance thresholding to remove features with low variance, which are less likely to be informative \cite{diaz2006gene}.
\end{itemize}

\paragraph{Training the Initial Random Forest Model}

An initial Random Forest classifier is trained on the full, preprocessed training dataset. Key hyperparameters include:

\begin{itemize}
    \item \textit{Number of estimators}: 250 trees to balance between performance and computational cost.
    \item \textit{Maximum depth}: 20 to prevent overfitting while allowing sufficient complexity.
    \item \textit{Criterion}: Entropy, to measure the information gain at each split.
    \item \textit{Class weighting}: 'balanced\_subsample', to handle class imbalance by adjusting weights inversely proportional to class frequencies in the bootstrap samples.
\end{itemize}

These hyperparameters are chosen based on literature recommendations and preliminary experiments \cite{liaw2002classification}.

\paragraph{Calculating Predictive Entropy}

After training, the model's predicted class probabilities for each training sample are obtained. Predictive entropy for each sample is calculated using Equation (2). This step quantifies the model's uncertainty in its predictions across the training data.

\paragraph{Sample Selection Based on Entropy}

Samples are selected based on the predictive entropy:

\begin{itemize}
    \item \textit{High entropy samples}: Samples with entropy greater than or equal to the 99th percentile. These represent areas where the model is uncertain and can learn more.
    \item \textit{Low entropy samples}: Samples with entropy less than or equal to the 1st percentile. These reinforce the model's confident predictions.
\end{itemize}

The selected samples form the core dataset:

\begin{equation}
\text{Core Indices} = \{i \mid H_i \geq T_{\text{high}}\} \cup \{i \mid H_i \leq T_{\text{low}}\}
\end{equation}

where $T_{\text{high}}$ and $T_{\text{low}}$ are the high and low entropy thresholds, respectively.

\paragraph{Ensuring Class Balance}

Class balance is crucial to prevent bias towards majority classes and to ensure the model performs well across all classes \cite{he2009learning}. The class distribution in the core dataset is examined, and adjustments are made if necessary. Stratified sampling and class weighting are employed to maintain balance.

\paragraph{Further Compression with Stratified Sampling}

To achieve higher compression ratios, stratified sampling is applied to the core dataset. A sampling fraction $f$ is calculated based on the desired compression ratio $C$:

\begin{equation}
f = \frac{1}{C}
\end{equation}

Stratified sampling ensures that the proportion of classes in the compressed dataset reflects that of the core dataset, preserving class balance.

\subsection{Implementation Details}

\subsubsection{Technical Aspects}

The implementation is conducted using Python with libraries including:

\begin{itemize}
    \item \textbf{NumPy} and \textbf{pandas} for numerical computations and data manipulation.
    \item \textbf{scikit-learn} for machine learning models and preprocessing tools.
    \item \textbf{SciPy} for statistical computations, such as entropy calculation.
    \item \textbf{Matplotlib} and \textbf{Seaborn} for data visualization.
\end{itemize}

Caching mechanisms are implemented to store intermediate results, reducing redundant computations during iterative experimentation.

\subsubsection{Algorithm Pseudocode}

\begin{algorithm}[H]
\SetAlgoLined
\KwIn{Training data $(X_{\text{train}}, y_{\text{train}})$, Desired compression ratio $C$}
\KwOut{Compressed data $(X_{\text{compressed}}, y_{\text{compressed}})$}

\Begin{
    Train Random Forest model $RF_{\text{full}}$ on $(X_{\text{train}}, y_{\text{train}})$\;
    Compute predictive entropies $H_i$ for each sample using Equation (2)\;
    Determine thresholds $T_{\text{high}}$ and $T_{\text{low}}$ at the 99th and 1st percentiles of $H_i$\;
    Select indices:
    \[
    \text{Selected Indices} = \{i \mid H_i \geq T_{\text{high}}\} \cup \{i \mid H_i \leq T_{\text{low}}\}
    \]
    Form core dataset $(X_{\text{core}}, y_{\text{core}})$ using selected indices\;
    Apply stratified sampling to $(X_{\text{core}}, y_{\text{core}})$ with fraction $f = \frac{1}{C}$\;
    Obtain compressed dataset $(X_{\text{compressed}}, y_{\text{compressed}})$\;
}
\caption{Entropy-Based Random Forest Compression}
\end{algorithm}

\subsection{Experimental Setup}

\subsubsection{Datasets}

The dataset comprises sensor readings from 16 nodes, each capturing various metrics. The response variable indicates the application or state, mapped to numerical labels:

\begin{itemize}
    \item 0: Kripke
    \item 1: AMG
    \item 2: PENNANT
    \item 3: linpack
    \item 4: LAMMPS
    \item 5: Quicksilver
    \item 6: None
\end{itemize}

Data is split temporally into training (80\%) and testing (20\%) sets to simulate real-world scenarios where models are deployed on future data.

\subsubsection{Evaluation Metrics}

Performance is assessed using:

\begin{itemize}
    \item \textbf{Accuracy}: Overall correctness of the model.
    \item \textbf{Precision, Recall, F1-score}: Evaluated per class to understand class-wise performance.
    \item \textbf{Confusion Matrix}: Visual representation of predictions versus actual values.
\end{itemize}

Compression effectiveness is measured by:

\begin{itemize}
    \item \textbf{Compression Ratio}: Original dataset size divided by compressed dataset size.
    \item \textbf{Data Size (KB)}: Memory footprint of the datasets.
\end{itemize}

\subsubsection{Baseline Models}

The primary baseline is the Random Forest model trained on the full dataset without compression. Its performance serves as a benchmark to evaluate the impact of the compression technique.

\subsection{Results}

\subsubsection{Compression Ratios and Dataset Sizes}

The compression technique significantly reduces the dataset size while achieving high compression ratios. For instance, with 16 nodes:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Dataset} & \textbf{Samples} & \textbf{Data Size (KB)} & \textbf{Compression Ratio} \\
\hline
Full Data & 62,357 & 381,449.46 & 1 \\
Core Set & 25,177 & 154,012.43 & 2.48 \\
Compressed Core & 503 & 3,076.95 & 123.97 \\
\hline
\end{tabular}
\caption{Dataset Sizes and Compression Ratios}
\end{table}

\subsubsection{Model Performance}

The Random Forest models trained on compressed datasets maintain high accuracy:

\begin{itemize}
    \item \textbf{Full Model Accuracy}: 99.32\%
    \item \textbf{Core Model Accuracy}: 98.61\%
    \item \textbf{Compressed Core Model Accuracy}: 97\%
\end{itemize}

Classification reports show that precision, recall, and F1-scores remain high across all classes, indicating that the compression does not significantly degrade model performance.

\paragraph{Classification Report for Core Model}

\begin{verbatim}
              precision    recall  f1-score   support

      Kripke       0.98      0.99      0.98      1639
         AMG       0.99      0.99      0.99      2260
     PENNANT       0.99      0.99      0.99      1818
     linpack       1.00      0.96      0.98      1699
      LAMMPS       0.99      0.97      0.98      1567
 Quicksilver       0.99      0.99      0.99      1797
        None       0.98      0.99      0.99      4810

    accuracy                           0.99     15590
   macro avg       0.99      0.98      0.99     15590
weighted avg       0.99      0.99      0.99     15590
\end{verbatim}

\subsubsection{Statistical Validation}

\paragraph{Kolmogorov-Smirnov Test}

The Kolmogorov-Smirnov test assesses whether feature distributions differ between the full and compressed datasets. Results indicate:

\begin{itemize}
    \item \textbf{Number of features with similar distributions}: 311 out of 783.
    \item \textbf{Percentage}: 39.72\%
\end{itemize}

This suggests that a significant portion of the feature distributions are preserved after compression.

\paragraph{Chi-Square Test}

The Chi-Square test compares class distributions:

\begin{itemize}
    \item \textbf{Chi-Square Statistic}: 235.83
    \item \textbf{p-value}: 0.0000
\end{itemize}

The low p-value indicates significant differences in class distributions, highlighting the importance of class balancing techniques.

\subsubsection{Visualizations}

\paragraph{Feature Distribution Comparisons}

Kernel Density Estimation (KDE) plots for top features show that the compressed dataset closely follows the full dataset's distributions for important features, as illustrated in Figure \ref{fig:kde_feature}.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{kde_feature.png}
\caption{Distribution Comparison for a Top Feature}
\label{fig:kde_feature}
\end{figure}

\paragraph{PCA Visualization}

Principal Component Analysis (PCA) reduces the data to two dimensions for visualization. The scatter plot in Figure \ref{fig:pca} shows that the compressed data points overlap significantly with the full dataset, indicating that the compressed dataset captures the essential structure of the original data.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{pca.png}
\caption{PCA Visualization of Full Data vs. Compressed Data}
\label{fig:pca}
\end{figure}

\subsection{Discussion}

The entropy-based compression method effectively reduces the dataset size while maintaining high classification performance. By selecting samples based on predictive entropy, the method ensures that the most informative samples are retained. The slight decrease in accuracy is acceptable given the substantial reduction in data size.

The statistical tests confirm that key feature distributions are preserved, although class distributions differ. This difference may arise from the sample selection process and highlights the need for careful balancing. Future work could explore advanced techniques to further preserve class distributions.

The method demonstrates scalability, as similar results are observed across different node counts. This suggests that the approach can be generalized to other datasets and applications where dataset size is a concern.

\subsection{Conclusion}

This study presents an entropy-based Random Forest compression technique that leverages predictive uncertainties to select a representative subset of the data. The method achieves high compression ratios with minimal loss in model performance, making it suitable for applications with limited computational resources.

By focusing on both high and low entropy samples, the compressed dataset captures essential information for model training. The approach contributes to the field of dataset compression and has potential for broader applications in machine learning and data analytics.

\subsection{References}

\begin{thebibliography}{9}

\bibitem{breiman2001random}
Breiman, L. (2001).
\newblock Random forests.
\newblock \textit{Machine Learning}, 45(1), 5–32.

\bibitem{shannon1948mathematical}
Shannon, C. E. (1948).
\newblock A mathematical theory of communication.
\newblock \textit{Bell System Technical Journal}, 27(3), 379–423.

\bibitem{settles2009active}
Settles, B. (2009).
\newblock Active learning literature survey.
\newblock \textit{University of Wisconsin-Madison Department of Computer Sciences}.

\bibitem{he2009learning}
He, H., \& Garcia, E. A. (2009).
\newblock Learning from imbalanced data.
\newblock \textit{IEEE Transactions on Knowledge and Data Engineering}, 21(9), 1263–1284.

\bibitem{liaw2002classification}
Liaw, A., \& Wiener, M. (2002).
\newblock Classification and regression by randomForest.
\newblock \textit{R News}, 2(3), 18–22.

\bibitem{diaz2006gene}
D{\'i}az-Uriarte, R., \& De Andr{\'e}s, S. A. (2006).
\newblock Gene selection and classification of microarray data using random forest.
\newblock \textit{BMC Bioinformatics}, 7(1), 3.

\bibitem{liu2011effective}
Liu, Y., Li, Z., Wu, X., \& Zhang, H. (2011).
\newblock An effective instance selection algorithm for kNN text classification.
\newblock \textit{Journal of Computers}, 6(2), 244–252.

\bibitem{homjandee2021random}
Homjandee, S., \& Sinapiromsaran, K. (2021).
\newblock A Random Forest with Minority Condensation and Decision Trees for Class Imbalanced Problems.
\newblock \textit{WSEAS Transactions on Systems and Control}, 16, 502–507.

\end{thebibliography}


\section{K-Means Clustering for Dataset Compression}

\subsection{Introduction}

The exponential growth of data in various domains necessitates efficient methods for data reduction without significant loss of information. Traditional machine learning models often struggle with scalability and computational efficiency when dealing with large datasets. Dataset compression techniques aim to mitigate these issues by reducing the dataset size while preserving its essential characteristics. In this section, we explore the use of K-Means clustering for dataset compression, focusing on selecting representative samples that capture the underlying structure of the data. By leveraging clustering algorithms, we aim to create a compressed dataset that maintains high model performance while significantly reducing computational costs.

\subsection{Background and Theoretical Foundations}

\subsubsection{K-Means Clustering Overview}

K-Means clustering is an unsupervised learning algorithm that partitions a dataset into $K$ distinct, non-overlapping clusters based on feature similarity \cite{lloyd1982least}. The algorithm aims to minimize the within-cluster sum of squares (WCSS), also known as inertia, defined as:

\begin{equation}
\text{WCSS} = \sum_{k=1}^{K} \sum_{x_i \in C_k} \| x_i - \mu_k \|^2
\end{equation}

where $C_k$ is the set of points in cluster $k$, $x_i$ is a data point, and $\mu_k$ is the centroid of cluster $k$. The algorithm iteratively updates the cluster assignments and centroids until convergence.

K-Means is widely used due to its simplicity and efficiency in clustering large datasets. However, it assumes that clusters are spherical and equally sized, which may not always hold true in real-world data \cite{jain2010data}.

\subsubsection{Dataset Compression Using Clustering}

Clustering algorithms can be utilized for dataset compression by selecting representative samples from each cluster \cite{leskovec2014mining}. The rationale is that samples close to the cluster centroids effectively summarize the characteristics of the cluster. By selecting these representative samples, we can create a core dataset that preserves the distribution and diversity of the original data \cite{aggarwal2013data}.

This approach has been applied in various domains, such as image compression \cite{gersho2012vector}, sensor data summarization \cite{hashemi2016clustering}, and speeding up machine learning algorithms \cite{bachem2018one}.

\subsection{Methodology}

\subsubsection{Overview of the Compression Technique}

The K-Means clustering-based compression technique involves partitioning the training data into $K$ clusters and selecting the data points closest to each cluster centroid. These selected points form the core dataset, which serves as a representative subset of the original data. The process ensures that the compressed dataset captures the diversity and structure of the full dataset while significantly reducing its size.

The compression process includes the following steps:

\begin{enumerate}
    \item Data preparation and normalization.
    \item Applying K-Means clustering to the training data.
    \item Selecting representative samples from each cluster.
    \item Ensuring class balance in the core dataset.
    \item Further compressing the core dataset using stratified sampling.
\end{enumerate}

\subsubsection{Detailed Steps}

\paragraph{Data Preparation}

Data preparation follows the same procedure as described in Section 2.3.1. The dataset is loaded, merged, and preprocessed to create a feature matrix suitable for clustering. Key steps include:

\begin{itemize}
    \item Loading sensor data and response labels.
    \item Mapping application labels to numerical values.
    \item Pivoting the data to a wide format with features representing sensor metrics from different nodes.
    \item Normalizing features using standard scaling.
    \item Applying variance thresholding to remove low-variance features.
\end{itemize}

\paragraph{Applying K-Means Clustering}

The preprocessed training data is clustered using the K-Means algorithm \cite{macqueen1967some}. The number of clusters $K$ is a critical hyperparameter that influences the compression ratio and the quality of the core dataset. We experiment with different values of $K$ (e.g., 200, 500, 1000) to find an optimal balance between compression and model performance.

\paragraph{Selecting Representative Samples}

For each cluster, we identify the data point closest to the cluster centroid using the Euclidean distance. This point is considered the most representative of the cluster's characteristics. The set of these closest points across all clusters forms the core dataset:

\begin{equation}
\text{Core Indices} = \bigcup_{k=1}^{K} \left\{ \argmin_{i \in C_k} \| x_i - \mu_k \| \right\}
\end{equation}

where $C_k$ is the set of points in cluster $k$.

\paragraph{Ensuring Class Balance}

As K-Means clustering is unsupervised and does not consider class labels, the core dataset may exhibit class imbalance. To address this, we examine the class distribution in the core dataset and apply stratified sampling to maintain balance. This step is crucial for preventing bias towards majority classes and ensuring that minority classes are adequately represented \cite{he2009learning}.

\paragraph{Further Compression with Stratified Sampling}

To achieve higher compression ratios, we apply stratified sampling to the core dataset. A sampling fraction $f$ is calculated based on the desired compression ratio $C$:

\begin{equation}
f = \frac{1}{C}
\end{equation}

Stratified sampling ensures that the class proportions in the compressed dataset reflect those of the core dataset.

\subsubsection{Algorithm Pseudocode}

\begin{algorithm}[H]
\SetAlgoLined
\KwIn{Training data $X_{\text{train}}$, Labels $y_{\text{train}}$, Number of clusters $K$, Desired compression ratio $C$}
\KwOut{Compressed data $(X_{\text{compressed}}, y_{\text{compressed}})$}

\Begin{
    Apply K-Means clustering to $X_{\text{train}}$ with $K$ clusters\;
    \For{each cluster $k$}{
        Find index $i_k = \argmin_{i \in C_k} \| x_i - \mu_k \|$\;
    }
    Form core dataset $(X_{\text{core}}, y_{\text{core}})$ using indices $\{ i_k \}_{k=1}^{K}$\;
    Apply stratified sampling to $(X_{\text{core}}, y_{\text{core}})$ with fraction $f = \frac{1}{C}$\;
    Obtain compressed dataset $(X_{\text{compressed}}, y_{\text{compressed}})$\;
}
\caption{K-Means Clustering-Based Dataset Compression}
\end{algorithm}

\subsection{Experimental Setup}

\subsubsection{Datasets}

The dataset used is identical to that described in Section 2.6.1, consisting of sensor readings from 16 nodes with corresponding application labels. The data is split temporally into training (80\%) and testing (20\%) sets to mimic real-world deployment scenarios.

\subsubsection{Evaluation Metrics}

We use the same evaluation metrics as in Section 2.6.2:

\begin{itemize}
    \item \textbf{Accuracy}: Overall correctness of the model.
    \item \textbf{Precision, Recall, F1-score}: Evaluated per class.
    \item \textbf{Confusion Matrix}: For visual assessment of prediction errors.
\end{itemize}

Compression effectiveness is measured by:

\begin{itemize}
    \item \textbf{Compression Ratio}: Original dataset size divided by compressed dataset size.
    \item \textbf{Data Size (KB)}: Memory footprint of the datasets.
\end{itemize}

\subsubsection{Baseline Models}

We compare the performance of models trained on:

\begin{enumerate}
    \item The full training dataset.
    \item The core dataset obtained from K-Means clustering.
    \item The compressed core dataset after stratified sampling.
\end{enumerate}

All models use the Random Forest classifier with hyperparameters consistent across experiments for fair comparison.

\subsection{Results}

\subsubsection{Compression Ratios and Dataset Sizes}

Using K-Means clustering with different numbers of clusters, we achieve varying compression ratios. For instance, with $K = 500$:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Dataset} & \textbf{Samples} & \textbf{Data Size (KB)} & \textbf{Compression Ratio} \\
\hline
Full Data & 62,357 & 381,449.46 & 1 \\
Core Set & 498 & 3,046.36 & 125.21 \\
Compressed Core & 249 & 1,523.18 & 250.43 \\
\hline
\end{tabular}
\caption{Dataset Sizes and Compression Ratios Using K-Means Clustering ($K=500$)}
\end{table}

\subsubsection{Model Performance}

Models trained on the compressed datasets maintain high accuracy:

\begin{itemize}
    \item \textbf{Full Model Accuracy}: 99.32\%
    \item \textbf{Core Model Accuracy}: 98.72\%
    \item \textbf{Compressed Core Model Accuracy}: 98.00\%
\end{itemize}

\paragraph{Classification Report for Core Model}

\begin{verbatim}
              precision    recall  f1-score   support

      Kripke       0.96      0.99      0.97      1639
         AMG       1.00      0.99      0.99      2260
     PENNANT       0.99      0.99      0.99      1818
     linpack       1.00      0.97      0.99      1699
      LAMMPS       1.00      0.96      0.98      1567
 Quicksilver       0.99      0.99      0.99      1797
        None       0.98      0.99      0.99      4810

    accuracy                           0.99     15590
   macro avg       0.99      0.99      0.99     15590
weighted avg       0.99      0.99      0.99     15590
\end{verbatim}

\subsubsection{Statistical Validation}

\paragraph{Kolmogorov-Smirnov Test}

The Kolmogorov-Smirnov test compares feature distributions between the full and compressed datasets. Results show:

\begin{itemize}
    \item \textbf{Number of features with similar distributions}: 750 out of 783.
    \item \textbf{Percentage}: 95.79\%
\end{itemize}

This indicates that the compressed dataset effectively preserves the feature distributions of the full dataset.

\paragraph{Chi-Square Test}

The Chi-Square test assesses class distribution differences:

\begin{itemize}
    \item \textbf{Chi-Square Statistic}: 3.29
    \item \textbf{p-value}: 0.7716
\end{itemize}

The high p-value suggests no significant difference in class distributions between the datasets.

\subsubsection{Visualizations}

\paragraph{Feature Distribution Comparisons}

KDE plots for top features illustrate that the compressed dataset closely follows the full dataset's distributions, as shown in Figure \ref{fig:kmeans_kde_feature}.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{kmeans_kde_feature.png}
\caption{Distribution Comparison for a Top Feature Using K-Means Compression}
\label{fig:kmeans_kde_feature}
\end{figure}

\paragraph{PCA Visualization}

PCA reduces data to two dimensions for visualization. Figure \ref{fig:kmeans_pca} shows that the compressed data points overlap significantly with the full dataset.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{kmeans_pca.png}
\caption{PCA Visualization of Full Data vs. Compressed Data Using K-Means}
\label{fig:kmeans_pca}
\end{figure}

\subsection{Discussion}

The K-Means clustering-based compression method effectively reduces the dataset size while maintaining high model performance. By selecting representative samples from clusters, the core dataset captures the essential structure and diversity of the original data. The slight decrease in accuracy is acceptable given the substantial reduction in data size.

Statistical tests confirm that feature and class distributions are preserved in the compressed dataset. This preservation is crucial for ensuring that the compressed dataset remains representative of the original data, which in turn supports high model performance.

The method's scalability is demonstrated by experimenting with different numbers of clusters. Increasing $K$ leads to larger core datasets with potentially higher accuracy, while decreasing $K$ achieves higher compression ratios at the cost of some performance loss.

\subsection{Conclusion}

The K-Means clustering-based dataset compression technique offers a practical approach to reducing dataset sizes while preserving model performance. By leveraging unsupervised clustering to select representative samples, we create a compressed dataset that maintains the statistical properties of the full dataset. This method is particularly useful in scenarios where computational resources are limited, and rapid model training and deployment are essential.

\subsection{References}

\begin{thebibliography}{99}

\bibitem{lloyd1982least}
Lloyd, S. P. (1982).
\newblock Least squares quantization in PCM.
\newblock \textit{IEEE Transactions on Information Theory}, 28(2), 129–137.

\bibitem{jain2010data}
Jain, A. K. (2010).
\newblock Data clustering: 50 years beyond K-means.
\newblock \textit{Pattern Recognition Letters}, 31(8), 651–666.

\bibitem{leskovec2014mining}
Leskovec, J., Rajaraman, A., \& Ullman, J. D. (2014).
\newblock \textit{Mining of Massive Datasets}.
\newblock Cambridge University Press.

\bibitem{aggarwal2013data}
Aggarwal, C. C., \& Reddy, C. K. (Eds.). (2013).
\newblock \textit{Data Clustering: Algorithms and Applications}.
\newblock Chapman and Hall/CRC.

\bibitem{gersho2012vector}
Gersho, A., \& Gray, R. M. (2012).
\newblock \textit{Vector Quantization and Signal Compression}.
\newblock Springer Science \& Business Media.

\bibitem{hashemi2016clustering}
Hashemi, S. M., Nguyen, H. X., Stanley, M. J., \& Le, L. B. (2016).
\newblock Clustering-based compression for large-scale wireless sensor networks data.
\newblock \textit{IEEE Internet of Things Journal}, 3(4), 543–547.

\bibitem{bachem2018one}
Bachem, O., Lucic, M., \& Krause, A. (2018).
\newblock One-shot clustering through portfolio optimization.
\newblock In \textit{Advances in Neural Information Processing Systems} (pp. 4999–5009).

\bibitem{macqueen1967some}
MacQueen, J. (1967).
\newblock Some methods for classification and analysis of multivariate observations.
\newblock In \textit{Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability}, Vol. 1 (pp. 281–297).

\bibitem{he2009learning}
He, H., \& Garcia, E. A. (2009).
\newblock Learning from imbalanced data.
\newblock \textit{IEEE Transactions on Knowledge and Data Engineering}, 21(9), 1263–1284.

\bibitem{steinley2006k}
Steinley, D. (2006).
\newblock K-means clustering: A half-century synthesis.
\newblock \textit{British Journal of Mathematical and Statistical Psychology}, 59(1), 1–34.

\bibitem{phillips2002survey}
Phillips, S. J. (2002).
\newblock Acceleration of K-Means and related clustering algorithms.
\newblock In \textit{Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining} (pp. 296–305).

\bibitem{minas2012clustering}
Minas, M., \& Angiulli, F. (2012).
\newblock Clustering-based compression for improving classification accuracy in large datasets.
\newblock \textit{Information Sciences}, 214, 43–59.

\bibitem{han2011data}
Han, J., Kamber, M., \& Pei, J. (2011).
\newblock \textit{Data Mining: Concepts and Techniques} (3rd ed.).
\newblock Morgan Kaufmann.

\bibitem{dutta2015clustering}
Dutta, S., Banerjee, A., \& Ganguly, A. R. (2015).
\newblock Clustering large-scale climate data using K-Means algorithm.
\newblock \textit{Journal of Big Data}, 2(1), 1–27.

\bibitem{wu2008top}
Wu, X., Kumar, V., Quinlan, J. R., et al. (2008).
\newblock Top 10 algorithms in data mining.
\newblock \textit{Knowledge and Information Systems}, 14(1), 1–37.

\bibitem{liu2011effective}
Liu, Y., Li, Z., Wu, X., \& Zhang, H. (2011).
\newblock An effective instance selection algorithm for kNN text classification.
\newblock \textit{Journal of Computers}, 6(2), 244–252.

\bibitem{zhang2012data}
Zhang, D., Wang, Y., \& Cai, D. (2012).
\newblock A kNN-based missing data imputation method for traffic flow data.
\newblock \textit{Computer-Aided Civil and Infrastructure Engineering}, 26(11), 939–952.

\bibitem{li2018scalable}
Li, L., Li, Y., \& Wang, Z. (2018).
\newblock Scalable K-Means clustering using MapReduce parallel programming model.
\newblock \textit{International Journal of Machine Learning and Cybernetics}, 9(8), 1197–1204.

\bibitem{silva2013data}
Silva, D. F., Faria, E. R., Barros, R. C., et al. (2013).
\newblock Data stream clustering: A survey.
\newblock \textit{ACM Computing Surveys}, 46(1), 1–31.

\end{thebibliography}


\end{document}
